name: Prompt Evaluations

on:
  pull_request:
    paths:
      - "**/*.md"
      - "!README.md"
      - "!CLAUDE.md"

# Set EVAL_BLOCK_MERGE=true in repo variables to make eval failures block merge
# Settings → Secrets and variables → Actions → Variables

jobs:
  eval:
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      EVAL_BLOCK_MERGE: ${{ vars.EVAL_BLOCK_MERGE || 'false' }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install promptfoo
        run: npm install -g promptfoo

      - name: Install Python dependencies
        run: pip install -e ".[dev]"

      - name: Find changed prompt files
        id: changed
        run: |
          FILES=$(git diff --name-only ${{ github.event.pull_request.base.sha }} HEAD -- '*.md' | grep -v README | grep -v CLAUDE | tr '\n' ' ')
          echo "files=$FILES" >> $GITHUB_OUTPUT
          if [ -z "$FILES" ]; then
            echo "No prompt files changed."
          else
            echo "Changed prompt files: $FILES"
          fi

      - name: Generate configs and run evaluations
        id: eval
        if: steps.changed.outputs.files != ''
        run: |
          set -e
          FAILED=0
          RESULTS_MD=""
          TOTAL_PASS=0
          TOTAL_FAIL=0
          FILES_EVALUATED=0

          for FILE in ${{ steps.changed.outputs.files }}; do
            echo "::group::Evaluating $FILE"

            # Use Python to parse front-matter and generate promptfoo config
            python -c "
          import sys, json, yaml
          from pathlib import Path
          from server.services.eval_service import generate_promptfoo_config
          from server.utils.front_matter import parse_front_matter

          content = Path('$FILE').read_text()
          fm, body = parse_front_matter(content)
          eval_cfg = fm.get('eval')
          if not eval_cfg and not body.strip():
              print('Skipping: no eval config or empty body')
              sys.exit(0)

          models = (eval_cfg or {}).get('models', ['gemini-2.0-flash'])
          config = generate_promptfoo_config(body, models, eval_cfg)
          Path('/tmp/promptfooconfig.yaml').write_text(yaml.dump(config, default_flow_style=False))
          print(f'Generated config for {len(models)} model(s)')
          "

            if [ -f /tmp/promptfooconfig.yaml ]; then
              FILES_EVALUATED=$((FILES_EVALUATED + 1))
              FILE_PASS=0
              FILE_FAIL=0

              promptfoo eval \
                --config /tmp/promptfooconfig.yaml \
                --output /tmp/eval-output.json \
                --no-cache \
                --no-progress-bar || FAILED=1

              if [ -f /tmp/eval-output.json ]; then
                # Parse results and build markdown summary
                FILE_RESULTS=$(python -c "
          import json, sys
          data = json.load(open('/tmp/eval-output.json'))
          results = data.get('results', data.get('table', {}).get('body', []))
          passes = 0
          fails = 0
          lines = []
          if isinstance(results, list):
              for r in results:
                  if isinstance(r, dict):
                      success = r.get('success', r.get('pass', False))
                      provider = r.get('provider', {})
                      if isinstance(provider, dict):
                          pname = provider.get('id', 'unknown')
                      else:
                          pname = str(provider)
                      if success:
                          passes += 1
                          lines.append(f'| {pname} | :white_check_mark: PASS |')
                      else:
                          fails += 1
                          lines.append(f'| {pname} | :x: FAIL |')
          print(f'{passes}|{fails}')
          for l in lines:
              print(l)
          ")
                FILE_PASS=$(echo "$FILE_RESULTS" | head -1 | cut -d'|' -f1)
                FILE_FAIL=$(echo "$FILE_RESULTS" | head -1 | cut -d'|' -f2)
                FILE_TABLE=$(echo "$FILE_RESULTS" | tail -n +2)

                TOTAL_PASS=$((TOTAL_PASS + FILE_PASS))
                TOTAL_FAIL=$((TOTAL_FAIL + FILE_FAIL))

                RESULTS_MD="${RESULTS_MD}
          ### \`${FILE}\`

          | Provider | Result |
          |----------|--------|
          ${FILE_TABLE}
          "
              fi
              rm -f /tmp/promptfooconfig.yaml /tmp/eval-output.json
            fi

            echo "::endgroup::"
          done

          # Save results for PR comment step
          echo "files_evaluated=$FILES_EVALUATED" >> $GITHUB_OUTPUT
          echo "total_pass=$TOTAL_PASS" >> $GITHUB_OUTPUT
          echo "total_fail=$TOTAL_FAIL" >> $GITHUB_OUTPUT
          echo "failed=$FAILED" >> $GITHUB_OUTPUT

          # Write markdown body to file (multiline output)
          cat > /tmp/eval-comment.md << 'COMMENT_EOF'
          ## Prompt Evaluation Results

          COMMENT_EOF

          if [ $FILES_EVALUATED -eq 0 ]; then
            echo "No prompts with eval config found in changed files." >> /tmp/eval-comment.md
          else
            if [ $TOTAL_FAIL -eq 0 ]; then
              echo ":white_check_mark: **All ${TOTAL_PASS} assertions passed** across ${FILES_EVALUATED} file(s)" >> /tmp/eval-comment.md
            else
              echo ":warning: **${TOTAL_FAIL} assertion(s) failed**, ${TOTAL_PASS} passed across ${FILES_EVALUATED} file(s)" >> /tmp/eval-comment.md
            fi
            echo "" >> /tmp/eval-comment.md
            echo "$RESULTS_MD" >> /tmp/eval-comment.md
          fi

          if [ "$EVAL_BLOCK_MERGE" = "true" ] && [ $FAILED -ne 0 ]; then
            echo "" >> /tmp/eval-comment.md
            echo "> :no_entry: Merge blocked — eval failures must be resolved. Set \`EVAL_BLOCK_MERGE=false\` in repo variables to disable." >> /tmp/eval-comment.md
          fi

      - name: Post PR comment
        if: always() && steps.changed.outputs.files != ''
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const body = fs.readFileSync('/tmp/eval-comment.md', 'utf8');
            const marker = '<!-- promptory-eval-results -->';

            // Find existing comment to update (avoids duplicate comments on re-runs)
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            const existing = comments.find(c => c.body.includes(marker));

            const fullBody = `${marker}\n${body}`;

            if (existing) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existing.id,
                body: fullBody,
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: fullBody,
              });
            }

      - name: Fail if merge blocking enabled and evals failed
        if: always() && steps.eval.outputs.failed == '1' && env.EVAL_BLOCK_MERGE == 'true'
        run: |
          echo "::error::Prompt evaluations failed and EVAL_BLOCK_MERGE is enabled"
          exit 1
